# -*- coding: utf-8 -*-
"""Helmet_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Il9uUVneCMZnH8Sr722nnTIQxHJi2Jhb
"""

# Install necessary libraries
!pip install ultralytics roboflow

# Import necessary libraries
from roboflow import Roboflow
from ultralytics import YOLO
import os

# Initialize Roboflow and download the dataset
rf = Roboflow(api_key="BuTdIp88ZV62txTaVg4n")
project = rf.workspace("miniproject-dfm83").project("helmetdetectiondataset")
version = project.version(7)
dataset = version.download("yolov5")

# Load a pretrained YOLOv5 model (using ultralytics YOLOv5s model)
model = YOLO("yolov5s.pt")

# Train the YOLOv5 model on your downloaded dataset
model.train(
    data=os.path.join(dataset.location, "data.yaml"),  # Path to data.yaml in the dataset folder
    epochs=50,       # Number of training epochs
    imgsz=640,       # Image size for training
    batch=16,        # Batch size
    name="helmet_detection_yolov5"  # Name for this training experiment
)

print("Training completed.")

from google.colab import drive
drive.mount('/content/drive')

# Save the model to Google Drive
save_path = "/content/drive/MyDrive/helmet_detection_model.pt"  # Specify your desired save location in Google Drive
model.save(save_path)

print("Model saved to Google Drive.")

from ultralytics import YOLO
from google.colab import drive
drive.mount('/content/drive')

# Path to the saved model in Google Drive
model_path = "/content/drive/MyDrive/helmet_detection_model.pt"  # Update if your path is different

# Load the model
loaded_model = YOLO(model_path)

print("Model loaded from Google Drive.")

import cv2
from ultralytics import YOLO
from google.colab.patches import cv2_imshow # Import the cv2_imshow function


# Load the YOLOv5 model
model = YOLO('/content/drive/MyDrive/helmet_detection_model.pt')  # Replace with your model path

# Load the sample video
video_path = '/content/9239005-hd_1920_1080_25fps.mp4'  # Replace with your video path
cap = cv2.VideoCapture(video_path)

# Check if video opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
    exit()

# Read and process video frames
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("End of video or cannot read the frame.")
        break

    # Perform inference on the frame
    results = model(frame)

    # Plot the detection results on the frame
    annotated_frame = results[0].plot()  # Get the annotated frame with detections

    # Display the frame with detections
    cv2_imshow(annotated_frame) # Use cv2_imshow instead of cv2.imshow

    # Break the loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release video capture object and close all OpenCV windows
cap.release()
cv2.destroyAllWindows()

import locale

def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

# Set the locale to UTF-8
!export LC_ALL=C.UTF-8
!export LANG=C.UTF-8

# Now install Gradio
!pip install gradio

import cv2
from ultralytics import YOLO
import gradio as gr
import tempfile
import os

# Load the YOLOv5 model
model = YOLO('/content/drive/MyDrive/helmet_detection_model.pt')  # Replace with your model path

def process_video(video_path):
    # Load the video file
    cap = cv2.VideoCapture(video_path)

    # Check if video opened successfully
    if not cap.isOpened():
        return "Error: Could not open video."

    # Prepare a temporary file to save the output video
    temp_output = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    output_video_path = temp_output.name

    # Get video properties
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initialize video writer
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    # Process each frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Perform inference on the frame
        results = model(frame)

        # Get the annotated frame with detections
        annotated_frame = results[0].plot()

        # Write the annotated frame to the output video
        out.write(annotated_frame)

    # Release video capture and writer objects
    cap.release()
    out.release()

    # Return the path to the output video for display
    return output_video_path

# Set up the Gradio interface
gr.Interface(
    fn=process_video,
    inputs=gr.Video(label="Upload a video for helmet detection"),
    outputs=gr.Video(label="Processed video with helmet detections"),
    title="Helmet Detection Video Processing",
    description="Upload a video to detect and annotate people with and without helmets using YOLO."
).launch()